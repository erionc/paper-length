
'''
Experiments with title, abstract, keywords, venue text fields independently
vectorized using TfIdf and publication year and number of citations joined as
numeric features. An MLP, a Linear Regressor, a Support Vector 
Regressor, a Random Forest, a Gradient Boosgin and and Extreme Gradient
Boosgin regressor are used as paper length predictors. 
'''

import pandas as pd
import numpy as np
import os, sys, argparse, json, re
from nltk import word_tokenize, sent_tokenize
from nltk.tokenize.treebank import TreebankWordTokenizer
from nltk.corpus import stopwords
stopWords = set(stopwords.words('english'))

from sklearn.feature_extraction.text import TfidfTransformer, TfidfVectorizer, CountVectorizer, HashingVectorizer
from sklearn.linear_model import LinearRegression
from sklearn.svm import SVR
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

from sklearn.neural_network import MLPRegressor
from sklearn.model_selection import GridSearchCV 
from sklearn.pipeline import FeatureUnion, Pipeline
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
import xgboost as xgb
from sklearn.compose import ColumnTransformer, make_column_transformer
from sklearn.preprocessing import MinMaxScaler, MaxAbsScaler

# just lowercase and ascii encode
def lower_key_string(text):
	# lowercase
	text = text.lower()
	# remove special characters by performing encode-decode in ascii
	text = text.encode('ascii', 'ignore').decode('ascii')
	return text

# takes list of unique keywords and returns keyword summary string
def summary_from_keywords(key_list):
	# remove any empty strings
	keys = [x for x in key_list if len(x) >= 1]
	# remove trailing spaces
	keys = [x.strip() for x in keys] 
	# remove duplicate keywords
	keys = list(set(keys))
	# generate and return comma-separated keyword string
	key_string = ' , '.join(keys)
	return key_string  

# function that tokenizes text same as Stanford CoreNLP
def core_tokenize(text):
	''' 
	Takes a text string and returns tokenized string using NLTK word_tokenize 
	same as in Stanford CoreNLP. space, \n \t are lost. "" are replace by ``''
	'''
	# tokenize | _ ^ / ~ + = * that are not tokenized by word_tokenize
	text = text.replace("|", " | ") ; text = text.replace("_", " _ ")
	text = text.replace("^", " ^ ") ; text = text.replace("/", " / ")
	text = text.replace("+", " + ") ; text = text.replace("=", " = ")
	text = text.replace("~", " ~ ") ; text = text.replace("*", " * ") 
   
	# tokenize with word_tokenize preserving lines similar to Stanford CoreNLP
	tokens = word_tokenize(text, preserve_line=True)

	for i, tok in enumerate(tokens):
		if tok == '...':
			continue
		# double match
		if re.match(r'[^.\s]{2,}\.[^.\s]{2,}', tok):
			tokens[i] = tok.replace('.', ' . ')
		# left match
		if re.match(r'[^.\s]{2,}\.', tok):
			tokens[i] = tok.replace('.', ' . ')
		# right match
		if re.match(r'\.[^.\s]{2,}', tok):
			tokens[i] = tok.replace('.', ' . ')

	# put all tokens together
	text = ' '.join(tokens)
	# remove double+ spaces
	text = re.sub(r'\s{2,}', " ", text)
	# lowercase
	text = text.lower()
	# remove special characters by performing encode-decode in ascii
	text = text.encode('ascii', 'ignore').decode('ascii')
	return text

# tokenizes text in ["abstract", "title", "keywords"] fields
def record_tokenize(rec):
	''' Tokenizes ALL fields of a dictionary or dataframe record with core_tokenize'''
	for k,v in rec.items():
		if k in ["abstract", "title"]:
			rec[k] = core_tokenize(v)
		elif k == "keywords":
			key_str = summary_from_keywords(v)
			rec[k] = lower_key_string(key_str)
	return rec

# read file json lines from given file path and return them in a list
def read_dicts_from_list(file_path):
	'''read json lines and store them in a list that is returned'''
	with open(file_path, "r", encoding='utf-8') as inf:   
		# strip \n at the end of each line
		line_list = [json.loads(line) for line in inf]
	return line_list

# write list records as lines in a given file path
def write_dicts_to_file(file_path, line_list):
	'''write list lines in a file path that is opened'''
	outf = open(file_path, "a", encoding='utf-8')  
	for itm in line_list:
		json.dump(itm, outf)
		outf.write('\n')
	outf.close()

# read the data
train_lst = read_dicts_from_list(os.path.join("data", "train.txt"))
val_lst = read_dicts_from_list(os.path.join("data", "val.txt"))
test_lst = read_dicts_from_list(os.path.join("data", "test.txt"))
full_sample_lst = train_lst + val_lst + test_lst

# tokenize, lowercase and convert list of keywords to string
full_sample_lst = [record_tokenize(rec) for rec in full_sample_lst]

# getting the targets
y = pd.DataFrame({"target": [int(s["plength"]) for s in full_sample_lst]})
y = np.ravel(y)

# getting the rest of fields
key_lst = [str(x["keywords"]) for x in full_sample_lst]
title_lst = [str(x["title"]) for x in full_sample_lst]
abst_lst = [str(x["abstract"]) for x in full_sample_lst]
venue_lst = [str(x["venue"]) for x in full_sample_lst]
year_lst = [int(x["year"]) for x in full_sample_lst]
cit_lst = [int(x["n_citation"]) for x in full_sample_lst]

# # putting all features together
X = pd.DataFrame({"keywords": key_lst, "title": title_lst, "abstract": abst_lst, 
	"venue": venue_lst, "year": year_lst, "citations": cit_lst}) 

parser = argparse.ArgumentParser()
parser.add_argument('-r', '--regressor', choices=['mlp', 'lr', 'svr', 'rf', 'gb', 'xgb'], help='Regression Model', required=True)
args = parser.parse_args()

if __name__ == '__main__': 

	# TFIDF vectorizer
	vect = TfidfVectorizer(lowercase=False)
	# create train - test splits
	y_train = y[:4000] ; y_test = y[4000:5000]
	X_train = X[:4000] ; X_test = X[4000:5000]

	# parameters of the vectorizer
	vect_grid = {
	'union__abstract__ngram_range': [(1, 1), (1,2), (1,3)],
	'union__abstract__norm': ['l1', 'l2', None], 
	'union__abstract__smooth_idf': [True, False], 
	'union__abstract__stop_words': [stopWords, None], 
	'union__abstract__sublinear_tf': [True, False]}

	# parameters for linear regression
	lr_grid = {
	'reg__copy_X': [True, False],
	'reg__fit_intercept': [True, False], 
	'reg__normalize': [True, False]}

	# parameters for support vector regression 
	svr_grid = {
	'reg__C': [0.01, 0.1, 1, 10, 100],
	'reg__gamma': ['scale', 'auto'], 
	'reg__kernel': ['linear', 'poly', 'rbf', 'sigmoid'],
	'reg__shrinking': [True, False]}

	# parameters for multilayer perceptron regressor 
	mlp_grid = {
	'reg__hidden_layer_sizes': [(50, ), (75, ), (100, ), (125, ), (150, )],
	'reg__alpha': [0.00005, 0.0001, 0.0005, 0.001, 0.005],
	'solver': ['lbfgs', 'sgd', 'adam']}

	# parameters for random forest regressor
	rf_grid = {
	'reg__n_estimators': [50, 60, 70, 85, 100, 150, 200],
	'reg__max_features': ["auto", "sqrt", "log2"], 
	'reg__bootstrap': [True, False], 'reg__oob_score': [True, False]}

	# parameters for gradient boosting regressor
	gb_grid = {
	'reg__n_estimators': [50, 60, 70, 85, 100, 150, 200],
	'reg__max_features': ['auto', 'sqrt', 'log2'],
	'reg__max_depth': [2, 3, 4, 5, 6, 7, 8]}

	# parameters for xg boosting regressor
	xgb_grid = {
	'reg__n_estimators': [50, 60, 70, 85, 100, 150, 200], 
	'reg__eta': [0.008, 0.015, 0.03, 0.05, 0.1, 0.15, 0.3],
	'reg__gamma': [0.002, 0.005, 0.01, 0.05, 0.1, 0.15, 0.2], 
	'reg__max_depth': [2, 3, 4, 5, 6, 7, 8]}

	# creating different regressors
	svr_model = SVR()
	lr_model = LinearRegression()
	mlp_model = MLPRegressor(random_state=7)
	rf_model = RandomForestRegressor(random_state=7, n_jobs=-1)
	gb_model = GradientBoostingRegressor(random_state=7)
	xgb_model = xgb.XGBRegressor(random_state=7)

	# selecting the regression model and the respective param grid
	if args.regressor.lower() == "mlp":
		model = mlp_model
		model_grid = mlp_grid 
	elif args.regressor.lower() == "lr":
		model = lr_model
		model_grid = lr_grid 
	elif args.regressor.lower() == "svr":
		model = svr_model
		model_grid = svr_grid 
	elif args.regressor.lower() == "rf":
		model = rf_model
		model_grid = rf_grid 
	elif args.regressor.lower() == "gb":
		model = gb_model
		model_grid = gb_grid 
	elif args.regressor.lower() == "xgb":
		model = xgb_model
		model_grid = xgb_grid 
	else:
		print("Wrong Regressor...")
		sys.exit()

	# creating pgrid joining vect_grid with model_grid
	pgrid = {**vect_grid, **model_grid}

	# the vectorizer and model pipeline
	pipe_model = Pipeline([
		('union', ColumnTransformer(
			[('keywords', vect, 0),
			('title', vect, 1),
			('abstract', vect, 2),
			('venue', vect, 3),
			], remainder='passthrough')),
		('reg', model)])

	# fit the model and prepare the gridsearch
	pipe_model.fit(X_train, y_train)
	# get the predictions from the default model
	y_pred_model = pipe_model.predict(X_test)

	gs = GridSearchCV(estimator=pipe_model, param_grid=pgrid, cv=4, n_jobs=-1, pre_dispatch=16, verbose=1)
	grid_result = gs.fit(X_train, y_train)
	best_model = grid_result.best_estimator_
	# get the predictions from the best model
	y_pred_grid = best_model.predict(X_test)
	
	# Print MSE MAE R2 scores for default and best model
	print(f"Default model scores:")
	print(f"MSE: {mean_squared_error(y_test, y_pred_model):.4f} \t MAE: {mean_absolute_error(y_test, y_pred_model):.4f} \t R2: {r2_score(y_test, y_pred_model):.4f}")
	print(f"Best model scores:")
	print(f"MSE: {mean_squared_error(y_test, y_pred_grid):.4f} \t MAE: {mean_absolute_error(y_test, y_pred_grid):.4f} \t R2: {r2_score(y_test, y_pred_grid):.4f}")
	print(f"Best model params:")
	print(f"{grid_result.best_params_}")